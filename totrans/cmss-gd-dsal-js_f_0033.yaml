- en: The Soul of Big O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve encountered O(N) and O(1), we begin to see that Big O notation
    does more than simply describe the number of steps an algorithm takes, such as
    with a hard number like 22 or 400\. Rather, it’s an answer to that key question
    on your forehead: if there are N data elements, how many steps will the algorithm
    take?'
  prefs: []
  type: TYPE_NORMAL
- en: While that key question is indeed the strict definition of Big O, there’s actually
    more to Big O than meets the eye.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have an algorithm that always takes three steps no matter how much
    data there is. That is, for N elements, the algorithm always takes three steps.
    How would you express that in terms of Big O?
  prefs: []
  type: TYPE_NORMAL
- en: Based on everything you’ve learned up to this point, you’d probably say that
    it’s O(3).
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s actually O(1). And that’s because of the next layer of understanding
    Big O, which I will reveal now.
  prefs: []
  type: TYPE_NORMAL
- en: While Big O is an expression of the number of an algorithm’s steps relative
    to N data elements, that alone misses the deeper why behind Big O, what I dub
    the “soul of Big O.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The soul of Big O is what Big O is truly concerned about: how will an algorithm’s
    performance change as the data increases?'
  prefs: []
  type: TYPE_NORMAL
- en: This is the soul of Big O. Big O doesn’t want to simply tell you how many steps
    an algorithm takes. It wants to tell you the story of how the number of steps
    increases as the data changes.
  prefs: []
  type: TYPE_NORMAL
- en: Viewed with this lens, we don’t care very much whether an algorithm is O(1)
    or O(3). Because both algorithms are the type that aren’t affected by increased
    data, as their number of steps remains constant, they’re essentially the same
    kind of algorithm. They’re both algorithms whose steps remain constant irrespective
    of the data, and we don’t care to make a distinction between the two.
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm that is O(N), on the other hand, is a different type of algorithm.
    It’s an algorithm whose performance is affected as we increase the data. More
    specifically, it’s the kind of algorithm whose steps increase in direct proportion
    to the data as the data increases. This is the story O(N) tells. It tells you
    about the proportional relationship between the data and the algorithm’s efficiency.
    It describes exactly how the number of steps increases as the data increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at how these two types of algorithms are plotted on a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/big_o_notation/graph_1.png](images/big_o_notation/graph_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that O(N) makes a perfect diagonal line. This is because for every additional
    piece of data, the algorithm takes one additional step. Accordingly, the more
    data, the more steps the algorithm will take.
  prefs: []
  type: TYPE_NORMAL
- en: Contrast this with O(1), which is a perfect horizontal line. No matter how much
    data there is, the number of steps remains constant.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper into the Soul of Big O
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see why the soul of Big O is so important, let’s go one level deeper. Say
    we had an algorithm of constant time that always took 100 steps no matter how
    much data there was. Would you consider that to be more or less performant than
    an algorithm that is O(N)?
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/big_o_notation/graph_2.png](images/big_o_notation/graph_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the graph depicts, for a data set that is fewer than 100 elements, an O(N)
    algorithm takes fewer steps than the O(1) 100-step algorithm. At exactly 100 elements,
    the lines cross, meaning the two algorithms take the same number of steps, namely
    100\. But here’s the key point: for all arrays greater than 100, the O(N) algorithm
    takes more steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Because there will always be some amount of data at which the tides turn, and
    O(N) takes more steps from that point until infinity, O(N) is considered to be,
    on the whole, less efficient than O(1) no matter how many steps the O(1) algorithm
    actually takes.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true even for an O(1) algorithm that always takes one million steps.
    As the data increases, there will inevitably reach a point at which O(N) becomes
    less efficient than the O(1) algorithm and will remain so up toward an infinite
    amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: Same Algorithm, Different Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you learned in the previous chapters, linear search isn’t always O(N). It’s
    true that if the item we’re looking for is in the final cell of the array, it
    will take N steps to find it. But when the item we’re searching for is found in
    the first cell of the array, linear search will find the item in just one step.
    So this case of linear search would be described as O(1). If we were to describe
    the efficiency of linear search in its totality, we’d say that linear search is
    O(1) in a best-case scenario and O(N) in a worst-case scenario.
  prefs: []
  type: TYPE_NORMAL
- en: While Big O effectively describes both the best- and worst-case scenarios of
    a given algorithm, Big O notation generally refers to the worst-case scenario
    unless specified otherwise. This is why most references will describe linear search
    as being O(N) even though it can be O(1) in a best-case scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because a “pessimistic” approach can be a useful tool: knowing exactly
    how inefficient an algorithm can get in a worst-case scenario prepares us for
    the worst and may have a strong impact on our choices.'
  prefs: []
  type: TYPE_NORMAL
