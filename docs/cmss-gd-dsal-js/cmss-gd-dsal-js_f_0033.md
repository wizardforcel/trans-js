## `Big O`的灵魂

现在我们遇到了`O(N)`和`O(1)`，开始看到`Big O`记号不仅仅描述算法所需的步骤数量，比如`22`或`400`这样具体的数字。相反，它回答了你脑海中那个关键的问题：如果有`N`个数据元素，算法将需要多少步骤？

虽然那个关键问题确实是`Big O`的严格定义，但实际上`Big O`比表面看到的要复杂得多。

假设我们有一个算法，无论数据量多少，它总是需要三步。也就是说，对于`N`个元素，该算法总是需要三步。你会如何用`Big O`来表示呢？

根据你到目前为止所学的内容，你可能会说它是`O(3)`。

然而，它实际上是`O(1)`。这正是我现在将揭示的理解`Big O`的下一层次。

虽然`Big O`是算法步骤相对于`N`个数据元素的表达，但单靠这一点无法揭示`Big O`背后的更深层原因，我称之为“`Big O`的灵魂”。

`Big O`的灵魂就是它真正关心的：随着数据的增加，算法的性能将如何变化？

这就是`Big O`的灵魂。`Big O`并不只是想告诉你一个算法需要多少步骤。它想要讲述的是当数据变化时，步骤数量如何增加的故事。

从这个角度看，我们并不太关心一个算法是`O(1)`还是`O(3)`。因为这两种算法都不受数据增加的影响，步骤数量保持不变，它们本质上是同一种算法。它们都是步骤数量与数据无关的算法，我们不在乎区分这两者。

另一方面，一个`O(N)`的算法是一种不同类型的算法。它的性能会随着数据的增加而受到影响。更具体地说，这种算法的步骤与数据成正比地增加。这就是`O(N)`所讲述的故事。它告诉你数据与算法效率之间的比例关系。它确切描述了随着数据增加，步骤数量如何增加。

看看这两种算法在图表上的绘制方式：

![images/big_o_notation/graph_1.png](images/big_o_notation/graph_1.png)

注意到`O(N)`形成了一条完美的对角线。这是因为每增加一条数据，算法就需要额外增加一步。因此，数据越多，算法所需的步骤也越多。

与此相比，`O(1)`是一条完美的水平线。无论数据量多大，步骤的数量始终保持不变。

### 深入探讨`Big O`的灵魂

为了理解`Big O`的核心为何如此重要，让我们深入探讨一下。假设我们有一个恒定时间的算法，无论数据量有多少，它始终需要100步。你认为这比`O(N)`算法更高效还是更低效？

请看以下图表：

![images/big_o_notation/graph_2.png](images/big_o_notation/graph_2.png)

正如图表所示，对于少于100个元素的数据集，`O(N)`算法的步骤少于`O(1)`的100步算法。在正好100个元素时，两条线交叉，这意味着两个算法所需的步骤相同，即100步。但关键点是：对于所有超过100的数组，`O(N)`算法所需的步骤更多。

因为总会有某个数据量使得局势逆转，从那时起`O(N)`的步骤将比`O(1)`更多，因此不管`O(1)`算法实际需要多少步，`O(N)`被认为整体上低效于`O(1)`。

对于一个始终需要一百万步的`O(1)`算法，情况也是如此。随着数据量的增加，必然会达到一个点，此时`O(N)`比`O(1)`算法效率更低，并且在数据量趋于无穷时依然如此。

### 同一算法，不同场景

正如你在前面的章节中所学，线性搜索并不总是`O(N)`。确实，如果我们要寻找的项目在数组的最后一个单元格，它将需要`N`步才能找到。但是当我们要寻找的项目在数组的第一个单元格时，线性搜索只需一步即可找到。因此，这种情况下的线性搜索可以被描述为`O(1)`。如果我们要描述线性搜索的整体效率，我们可以说在最佳情况下线性搜索是`O(1)`，而在最坏情况下是`O(N)`。

虽然`Big O`有效地描述了给定算法的最佳和最坏情况，但`Big O`符号通常指的是最坏情况，除非另有说明。这就是为什么大多数参考资料将线性搜索描述为`O(N)`，即使在最佳情况下它可以是`O(1)`。

这是因为“悲观”的方法可以成为一个有用的工具：确切知道在最坏情况下一个算法会有多低效，可以让我们为最坏情况做好准备，并可能对我们的选择产生重大影响。
